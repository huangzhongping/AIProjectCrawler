"""
å®Œæ•´æµæ°´çº¿é›†æˆæµ‹è¯•
"""

import pytest
import asyncio
from pathlib import Path
from unittest.mock import patch, AsyncMock

# å¯¼å…¥ä¸»è¦æ¨¡å—
from utils.config import load_config
from utils.data_cleaner import DataCleaner
from utils.storage import DataStorage
from ai_analysis.classifier import AIProjectClassifier
from ai_analysis.keyword_extractor import KeywordExtractor
from ai_analysis.summarizer import ProjectSummarizer
from visualization.chart_generator import ChartGenerator
from visualization.report_generator import ReportGenerator


class TestFullPipeline:
    """å®Œæ•´æµæ°´çº¿æµ‹è¯•"""
    
    @pytest.fixture
    def pipeline_config(self, test_config, temp_dir):
        """æµæ°´çº¿æµ‹è¯•é…ç½®"""
        # ç¡®ä¿æ‰€æœ‰è·¯å¾„éƒ½æŒ‡å‘ä¸´æ—¶ç›®å½•
        config = test_config.copy()
        config['data']['paths'] = {
            'raw_data': str(temp_dir / 'raw'),
            'processed_data': str(temp_dir / 'processed'),
            'archive_data': str(temp_dir / 'archive'),
            'output': str(temp_dir / 'output')
        }
        
        # åˆ›å»ºç›®å½•
        for path in config['data']['paths'].values():
            Path(path).mkdir(parents=True, exist_ok=True)
        
        return config
    
    def test_data_processing_pipeline(self, pipeline_config, sample_projects):
        """æµ‹è¯•æ•°æ®å¤„ç†æµæ°´çº¿"""
        # 1. æ•°æ®æ¸…æ´—
        cleaner = DataCleaner(pipeline_config)
        cleaned_data = cleaner.clean_and_deduplicate(sample_projects)
        
        assert len(cleaned_data) <= len(sample_projects)  # å¯èƒ½æœ‰å»é‡
        assert all('cleaned_at' in project for project in cleaned_data)
        
        # 2. æ•°æ®å­˜å‚¨
        storage = DataStorage(pipeline_config)
        storage.save_daily_data(cleaned_data, '2023-12-01')
        
        # éªŒè¯æ•°æ®ä¿å­˜
        saved_data = storage.get_projects_by_date('2023-12-01')
        assert len(saved_data) == len(cleaned_data)
        
        # éªŒè¯ç»Ÿè®¡æ•°æ®
        stats = storage.get_daily_stats('2023-12-01')
        assert stats is not None
        assert stats['total_projects'] == len(cleaned_data)
    
    @pytest.mark.asyncio
    async def test_ai_analysis_pipeline(self, pipeline_config, sample_projects):
        """æµ‹è¯•AIåˆ†ææµæ°´çº¿"""
        # ä½¿ç”¨å…³é”®è¯åˆ†æï¼ˆä¸éœ€è¦APIï¼‰
        pipeline_config['api']['openai']['api_key'] = '${OPENAI_API_KEY}'
        
        # 1. AIåˆ†ç±»
        classifier = AIProjectClassifier(pipeline_config)
        ai_projects = []
        
        for project in sample_projects:
            classification = await classifier.classify(project)
            if classification['is_ai_related']:
                project['ai_classification'] = classification
                ai_projects.append(project)
        
        assert len(ai_projects) > 0  # åº”è¯¥æœ‰AIé¡¹ç›®
        
        # 2. å…³é”®è¯æå–
        extractor = KeywordExtractor(pipeline_config)
        for project in ai_projects:
            keywords = await extractor.extract(project)
            project['keywords'] = keywords
            assert 'keywords' in keywords
            assert isinstance(keywords['keywords'], list)
        
        # 3. é¡¹ç›®æ€»ç»“
        summarizer = ProjectSummarizer(pipeline_config)
        for project in ai_projects:
            summary = await summarizer.summarize(project)
            project['summary'] = summary
            assert 'summary' in summary
            assert len(summary['summary']) > 0
    
    def test_visualization_pipeline(self, pipeline_config, sample_projects):
        """æµ‹è¯•å¯è§†åŒ–æµæ°´çº¿"""
        # å‡†å¤‡å¸¦æœ‰AIåˆ†æç»“æœçš„é¡¹ç›®æ•°æ®
        processed_projects = []
        for project in sample_projects:
            project.update({
                'ai_classification': {
                    'is_ai_related': True,
                    'confidence_score': 0.8,
                    'ai_categories': ['Machine Learning']
                },
                'keywords': {
                    'keywords': ['ai', 'machine learning', 'python'],
                    'categories': {'æŠ€æœ¯æ ˆ': ['python'], 'åº”ç”¨é¢†åŸŸ': ['ai']}
                },
                'summary': {
                    'summary': 'è¿™æ˜¯ä¸€ä¸ªAIé¡¹ç›®',
                    'highlights': ['ä½¿ç”¨æœºå™¨å­¦ä¹ ', 'å¼€æºé¡¹ç›®'],
                    'use_cases': ['æ•°æ®åˆ†æ', 'è‡ªåŠ¨åŒ–']
                }
            })
            processed_projects.append(project)
        
        # 1. å›¾è¡¨ç”Ÿæˆ
        chart_generator = ChartGenerator(pipeline_config)
        charts = chart_generator.generate_daily_charts(processed_projects)
        
        assert isinstance(charts, dict)
        assert len(charts) > 0
        
        # éªŒè¯å›¾è¡¨æ–‡ä»¶å­˜åœ¨
        for chart_name, chart_path in charts.items():
            if chart_path:  # æŸäº›å›¾è¡¨å¯èƒ½ç”Ÿæˆå¤±è´¥
                assert Path(chart_path).exists()
    
    @pytest.mark.asyncio
    async def test_report_generation_pipeline(self, pipeline_config, sample_projects):
        """æµ‹è¯•æŠ¥å‘Šç”Ÿæˆæµæ°´çº¿"""
        # å‡†å¤‡æ•°æ®
        processed_projects = []
        for project in sample_projects:
            project.update({
                'ai_classification': {
                    'is_ai_related': True,
                    'confidence_score': 0.8,
                    'ai_categories': ['Machine Learning']
                },
                'keywords': {
                    'keywords': ['ai', 'machine learning', 'python'],
                    'categories': {'æŠ€æœ¯æ ˆ': ['python'], 'åº”ç”¨é¢†åŸŸ': ['ai']}
                },
                'summary': {
                    'summary': 'è¿™æ˜¯ä¸€ä¸ªAIé¡¹ç›®',
                    'highlights': ['ä½¿ç”¨æœºå™¨å­¦ä¹ ', 'å¼€æºé¡¹ç›®'],
                    'use_cases': ['æ•°æ®åˆ†æ', 'è‡ªåŠ¨åŒ–']
                }
            })
            processed_projects.append(project)
        
        # ç”Ÿæˆå›¾è¡¨
        chart_generator = ChartGenerator(pipeline_config)
        charts = chart_generator.generate_daily_charts(processed_projects)
        
        # ç”ŸæˆæŠ¥å‘Š
        pipeline_config['api']['openai']['api_key'] = '${OPENAI_API_KEY}'  # ä½¿ç”¨åŸºç¡€åˆ†æ
        report_generator = ReportGenerator(pipeline_config)
        report = await report_generator.generate_daily_report(processed_projects, charts)
        
        assert 'html' in report
        assert 'markdown' in report
        assert 'data' in report
        
        # éªŒè¯HTMLæŠ¥å‘Šå†…å®¹
        html_content = report['html']
        assert 'AIçˆ†æ¬¾é¡¹ç›®é›·è¾¾' in html_content
        assert len(html_content) > 1000  # åº”è¯¥æ˜¯å®Œæ•´çš„HTML
        
        # éªŒè¯MarkdownæŠ¥å‘Šå†…å®¹
        md_content = report['markdown']
        assert '# ğŸš€ AIçˆ†æ¬¾é¡¹ç›®é›·è¾¾' in md_content
        assert '## ğŸ“Š ä»Šæ—¥æ¦‚è§ˆ' in md_content
    
    @pytest.mark.asyncio
    async def test_end_to_end_pipeline(self, pipeline_config, sample_projects):
        """æµ‹è¯•ç«¯åˆ°ç«¯æµæ°´çº¿"""
        # æ¨¡æ‹Ÿå®Œæ•´çš„æ¯æ—¥æ›´æ–°æµç¨‹
        
        # 1. æ•°æ®æ¸…æ´—
        cleaner = DataCleaner(pipeline_config)
        cleaned_data = cleaner.clean_and_deduplicate(sample_projects)
        
        # 2. AIåˆ†æï¼ˆä½¿ç”¨å…³é”®è¯åˆ†æï¼‰
        pipeline_config['api']['openai']['api_key'] = '${OPENAI_API_KEY}'
        
        classifier = AIProjectClassifier(pipeline_config)
        extractor = KeywordExtractor(pipeline_config)
        summarizer = ProjectSummarizer(pipeline_config)
        
        ai_projects = []
        for project in cleaned_data:
            # AIåˆ†ç±»
            classification = await classifier.classify(project)
            if classification['is_ai_related']:
                # å…³é”®è¯æå–
                keywords = await extractor.extract(project)
                # é¡¹ç›®æ€»ç»“
                summary = await summarizer.summarize(project)
                
                project.update({
                    'ai_classification': classification,
                    'keywords': keywords,
                    'summary': summary
                })
                ai_projects.append(project)
        
        assert len(ai_projects) > 0
        
        # 3. æ•°æ®å­˜å‚¨
        storage = DataStorage(pipeline_config)
        storage.save_daily_data(ai_projects, '2023-12-01')
        
        # 4. å¯è§†åŒ–
        chart_generator = ChartGenerator(pipeline_config)
        charts = chart_generator.generate_daily_charts(ai_projects)
        
        report_generator = ReportGenerator(pipeline_config)
        report = await report_generator.generate_daily_report(ai_projects, charts)
        
        # 5. éªŒè¯æœ€ç»ˆç»“æœ
        assert len(ai_projects) <= len(sample_projects)
        assert all('ai_classification' in p for p in ai_projects)
        assert all('keywords' in p for p in ai_projects)
        assert all('summary' in p for p in ai_projects)
        
        # éªŒè¯æŠ¥å‘Šç”Ÿæˆ
        assert 'html' in report
        assert 'markdown' in report
        assert len(report['html']) > 1000
        assert len(report['markdown']) > 500
        
        # éªŒè¯æ•°æ®åº“å­˜å‚¨
        saved_projects = storage.get_projects_by_date('2023-12-01')
        assert len(saved_projects) == len(ai_projects)
        
        stats = storage.get_daily_stats('2023-12-01')
        assert stats['ai_projects'] == len(ai_projects)
    
    def test_error_handling_pipeline(self, pipeline_config):
        """æµ‹è¯•é”™è¯¯å¤„ç†æµæ°´çº¿"""
        # æµ‹è¯•ç©ºæ•°æ®å¤„ç†
        cleaner = DataCleaner(pipeline_config)
        empty_result = cleaner.clean_and_deduplicate([])
        assert empty_result == []
        
        # æµ‹è¯•æ— æ•ˆæ•°æ®å¤„ç†
        invalid_data = [
            {'name': ''},  # æ— æ•ˆåç§°
            {'description': 'test'},  # ç¼ºå°‘åç§°
            {}  # ç©ºæ•°æ®
        ]
        
        cleaned = cleaner.clean_and_deduplicate(invalid_data)
        assert len(cleaned) == 0  # æ‰€æœ‰æ•°æ®éƒ½åº”è¯¥è¢«è¿‡æ»¤æ‰
        
        # æµ‹è¯•å­˜å‚¨é”™è¯¯å¤„ç†
        storage = DataStorage(pipeline_config)
        
        # æµ‹è¯•è·å–ä¸å­˜åœ¨çš„æ•°æ®
        non_existent = storage.get_projects_by_date('1900-01-01')
        assert non_existent == []
        
        stats = storage.get_daily_stats('1900-01-01')
        assert stats is None
    
    def test_configuration_validation(self, temp_dir):
        """æµ‹è¯•é…ç½®éªŒè¯"""
        from utils.config import validate_config, create_directories
        
        # æµ‹è¯•æœ‰æ•ˆé…ç½®
        valid_config = {
            'api': {'openai': {'api_key': 'test-key'}},
            'crawler': {'request': {'timeout': 30}},
            'data': {'paths': {'raw_data': str(temp_dir / 'raw')}},
            'logging': {'level': 'INFO'}
        }
        
        assert validate_config(valid_config) is True
        
        # æµ‹è¯•åˆ›å»ºç›®å½•
        create_directories(valid_config)
        assert (temp_dir / 'raw').exists()
        
        # æµ‹è¯•æ— æ•ˆé…ç½®
        invalid_config = {}
        assert validate_config(invalid_config) is False
